{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c040df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# many-to-many RNN\n",
    "\n",
    "# 품사 태깅, 개체명 인식 등에 사용됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30611dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', 'a', 'e', 'l', 'p']\n",
      "문자 집합의 크기 : 5\n",
      "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n",
      "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n",
      "[1, 4, 4, 3, 2]\n",
      "[4, 4, 3, 2, 0]\n",
      "[[1, 4, 4, 3, 2]]\n",
      "[[4, 4, 3, 2, 0]]\n",
      "[array([[0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0.]])]\n",
      "훈련 데이터의 크기 : torch.Size([1, 5, 5])\n",
      "레이블의 크기 : torch.Size([1, 5])\n",
      "torch.Size([1, 5, 5])\n",
      "torch.Size([5, 5])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([5])\n",
      "0 loss:  1.7598316669464111 prediction:  [[3 3 3 3 3]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  lllll\n",
      "1 loss:  1.4910593032836914 prediction:  [[2 3 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  elle!\n",
      "2 loss:  1.290828824043274 prediction:  [[2 3 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  elle!\n",
      "3 loss:  1.075850248336792 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "4 loss:  0.870538055896759 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "5 loss:  0.694575846195221 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "6 loss:  0.5216812491416931 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "7 loss:  0.3715974688529968 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "8 loss:  0.25956013798713684 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "9 loss:  0.17975011467933655 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "10 loss:  0.12729187309741974 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "11 loss:  0.09329478442668915 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "12 loss:  0.06986822187900543 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "13 loss:  0.05309011787176132 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "14 loss:  0.04108026996254921 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "15 loss:  0.03247976303100586 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "16 loss:  0.026229405775666237 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "17 loss:  0.02158796787261963 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "18 loss:  0.01806631125509739 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "19 loss:  0.015342648141086102 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "20 loss:  0.013200494460761547 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "21 loss:  0.011490521021187305 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "22 loss:  0.010107279755175114 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "23 loss:  0.008974769152700901 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "24 loss:  0.008037344552576542 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "25 loss:  0.0072537632659077644 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "26 loss:  0.006592852063477039 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "27 loss:  0.006030945107340813 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "28 loss:  0.005549796856939793 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "29 loss:  0.005135026760399342 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "30 loss:  0.004775354638695717 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "31 loss:  0.004461791832000017 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "32 loss:  0.004187089391052723 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "33 loss:  0.003945182077586651 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "34 loss:  0.0037314079236239195 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "35 loss:  0.0035415408201515675 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "36 loss:  0.003372387494891882 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "37 loss:  0.003221128135919571 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "38 loss:  0.0030852188356220722 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "39 loss:  0.002962825121358037 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "40 loss:  0.002852108096703887 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "41 loss:  0.002751630265265703 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "42 loss:  0.0026601410936564207 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "43 loss:  0.0025766263715922832 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "44 loss:  0.0025000940077006817 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "45 loss:  0.0024296932388097048 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "46 loss:  0.0023647858761250973 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "47 loss:  0.002304782159626484 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "48 loss:  0.0022490662522614002 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "49 loss:  0.002197141060605645 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "50 loss:  0.0021487469784915447 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "51 loss:  0.0021034576930105686 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "52 loss:  0.0020608704071491957 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "53 loss:  0.002020820276811719 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "54 loss:  0.001983046066015959 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "55 loss:  0.0019472871208563447 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "56 loss:  0.0019133780151605606 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "57 loss:  0.0018811760237440467 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "58 loss:  0.001850445056334138 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "59 loss:  0.0018211131682619452 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "60 loss:  0.0017929909517988563 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "61 loss:  0.001766007044352591 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "62 loss:  0.001740067033097148 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "63 loss:  0.0017150761559605598 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "64 loss:  0.001690938719548285 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "65 loss:  0.0016675606602802873 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "66 loss:  0.0016448941314592957 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "67 loss:  0.0016229155007749796 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "68 loss:  0.00160148274153471 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "69 loss:  0.001580595038831234 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "70 loss:  0.0015602533239871264 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "71 loss:  0.0015402433928102255 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "72 loss:  0.0015206842217594385 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "73 loss:  0.001501480583101511 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "74 loss:  0.001482585328631103 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "75 loss:  0.0014639029977843165 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "76 loss:  0.001445553032681346 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "77 loss:  0.001427320996299386 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "78 loss:  0.0014092783676460385 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "79 loss:  0.0013914728770032525 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "80 loss:  0.0013736665714532137 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 loss:  0.0013560022925958037 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "82 loss:  0.00133845629170537 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "83 loss:  0.001320861978456378 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "84 loss:  0.001303266966715455 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "85 loss:  0.0012856714893132448 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "86 loss:  0.0012680275831371546 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "87 loss:  0.0012502879835665226 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "88 loss:  0.0012324047274887562 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "89 loss:  0.0012144260108470917 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "90 loss:  0.0011962081771343946 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "91 loss:  0.0011778711341321468 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "92 loss:  0.0011592477094382048 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "93 loss:  0.0011403856333345175 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "94 loss:  0.001121237175539136 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "95 loss:  0.0011018025688827038 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "96 loss:  0.0010820099851116538 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "97 loss:  0.0010618835221976042 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "98 loss:  0.0010414228308945894 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "99 loss:  0.001020532799884677 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
     ]
    }
   ],
   "source": [
    "# 문자 단위 RNN(Char RNN)\n",
    "# RNN의 입출력 단위가 문자인 경우\n",
    "\n",
    "# 문자 시퀀스 apple을 입력받으면 pple!를 출력하는 RNN을 구현\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# 훈련 데이터 전처리하기\n",
    "input_str = 'apple'\n",
    "label_str = 'pple!'\n",
    "char_vocab = sorted(list(set(input_str+label_str)))  # 문자 집합 만들기\n",
    "print(char_vocab)\n",
    "vocab_size = len(char_vocab)\n",
    "print ('문자 집합의 크기 : {}'.format(vocab_size))\n",
    "\n",
    "# 하이퍼파라미터 정의\n",
    "# 원-핫 벡터를 사용할 것이므로 입력의 크기는 문자 집합 크기와 같아야 함\n",
    "input_size = vocab_size  # 입력 크기 = 문자 집합의 크기\n",
    "hidden_size = 5\n",
    "output_size = 5\n",
    "learning_rate = 0.1\n",
    "\n",
    "char_to_index = dict((c, i) for i, c in enumerate(char_vocab)) # 문자 집합에 고유한 정수를 부여\n",
    "print(char_to_index)\n",
    "index_to_char = {}  # 예측 결과를 다시 문자로 보기 위한 것\n",
    "for key, value in char_to_index.items():\n",
    "    index_to_char[value] = key\n",
    "print(index_to_char)\n",
    "\n",
    "# 입력 데이터와 레이블 데이터 매핑\n",
    "x_data = [char_to_index[c] for c in input_str]\n",
    "y_data = [char_to_index[c] for c in label_str]\n",
    "print(x_data)\n",
    "print(y_data)\n",
    "\n",
    "# 배치 차원 추가: nn.RNN()은 3차원 텐서를 입력으로 받음\n",
    "x_data  = [x_data]\n",
    "y_data = [y_data]\n",
    "print(x_data)\n",
    "print(y_data)\n",
    "\n",
    "# 입력 시퀀스의 각 문자들을 one-hot벡터로 바꿔줌\n",
    "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
    "print(x_one_hot)\n",
    "\n",
    "# 데이터 텐서로 변환\n",
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)\n",
    "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
    "print('레이블의 크기 : {}'.format(Y.shape))\n",
    "\n",
    "# RNN 모델 구현하기\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "net = Net(input_size, hidden_size, output_size)\n",
    "outputs = net(X)\n",
    "print(outputs.shape) # 3차원 텐서 (배치 차원, 시점(timesteps), 출력)\n",
    "# 정확도를 측정할 때는 이를 모두 펼쳐서 계산\n",
    "# view를 사용하여 배치 차원과 시점 차원을 하나로 만들기\n",
    "print(outputs.view(-1, input_size).shape) # 2차원 텐서로 변환\n",
    "\n",
    "print(Y.shape)\n",
    "print(Y.view(-1).shape)\n",
    "\n",
    "# 옵티마이저와 손실 함수 정의\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)\n",
    "\n",
    "# 학습진행\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X)\n",
    "    loss = criterion(outputs.view(-1, input_size), Y.view(-1))  # Batch 차원 제거\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    result = outputs.data.numpy().argmax(axis=2) # 최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\n",
    "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
    "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d911dfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'h': 0, 'b': 1, 't': 2, 'o': 3, 'k': 4, 'u': 5, \"'\": 6, 'a': 7, 'y': 8, ',': 9, ' ': 10, 'n': 11, 'w': 12, 'p': 13, '.': 14, 'f': 15, 'm': 16, 'r': 17, 'g': 18, 'd': 19, 'c': 20, 'e': 21, 'i': 22, 'l': 23, 's': 24}\n",
      "[22, 15, 10, 8, 3, 5, 10, 12, 7, 11]\n",
      "[15, 10, 8, 3, 5, 10, 12, 7, 11, 2]\n",
      "훈련 데이터의 크기 : torch.Size([170, 10, 25])\n",
      "레이블의 크기 : torch.Size([170, 10])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([15, 10,  8,  3,  5, 10, 12,  7, 11,  2])\n",
      "ooooooboboooooeooooooooooooooaoobooeooobobooobeooeooooeooeeobboboooooooeeeoooeooooboobeeooobooooeobeoooooeoooeooooioooioooooooyeoooooobooeoooobooeeoooeooeooeeeoooioeooooooooooeoob\n",
      "aato  tofoso ote t to o e   to ot o o ot otoofe otoo ofe oto tofo oaoto ottooteooie o oe  otee ee o e o ooeoo tooo so oee otooteetoe otoote otooo toe tofo ooot oe oe  otoot e to t\n",
      "       e e e                                                          e                                                                                                            \n",
      "tlphntmhwentteletle tttetpettenettte ttetetetwetae ettett etttettenttteaetttettetetetettmhettetltet eeteteteteteletetett t ettwetme tttttnettetet ttsttemmnetttelettetn etttetttemt\n",
      "ttt ln tn tnnltornntn toam taolntno tn toatoamttoatnnahoattoa tn ttn toat noa n rn tnntnttnoonr torttnantnnoannonnloa totn toattoa tnnloa  n  tortootntoa tna talnnatnnatoatoan tnn\n",
      "to    to   e lh t   t t t   t t  t  e lt  tot  t     tt   to     t   e    t   t t tea   t       tot   t t t     s t t t e  to   et e  to t t lt    oe ten to  t   t  t lo lt  n ea \n",
      " oeo  to   e   e  toe  oe  eoe  toe    to tea the  e the  to   a     eo   oot eo   oe      eoe  eo         o  toa   eo  e  to   o  e  to   a eho  the  e       o  e    e        o  \n",
      "  toa tou  ao  oa toa the  ao u theo   a  pea  h     them ton oa he  to  eaotheo  the   e  ho   to    e   eo  thto  eot eu to  tothe  to  ea thom tha taa     t            tha toa \n",
      "  ton  o   ao  o    d t e   tod toe    t  tod  t       er to      n  to   to  to  t e      to   to        eo  t t   t t e  to  d t em to      to  to   ad d   t   n  n  d  t       \n",
      " pton  oem to     d uot e   ton t t nm t  t d  tmt d t en ton     n  to   tod tod totm  i  toer toe   to  tog t t   t t  n ton  et em to   n  tod toemt d d s t   d  ep tn toem  n \n",
      " pthnmtoem torcon d aothem  ton'tht nm tneperp emton t er toncon  nm tnrm tod ton'thtms ic toem todp  tnd ton t tot tothem tonchethem tonc n' ton toemtod d t dn  d  ep tn themtonm\n",
      " ptodmtoem aod'or h aothem  tor'toapim ametorp imtorlt er toncor   m eorm apd aor'toams gc toem todps tnd aor't dor aother ton'hethem todcer' tor toemtod a s ams ns ec an themtodm\n",
      "lptonltosm aoslo ld aother, tor'toalsm am torm imthslt er th  o le t eosl asd aor'thams ls toem tos s aod aorlt tot aothem th  uethem ths o l tor themtod a s ems ns nc ao themthem\n",
      "ly onltosm ao lo ld aothem, tor'tha em ap torm elthslther th lo le t ao l aod aor'tha s lm toem tosls end aorl, dot aothem to lhethem to ler'efor toemtod e s els n' ny po them oem\n",
      "ly on'tosm ao lom d todhem, tor'todoem an pert enth  them th lo lent wonm aod ton'tod s gm toem to k  and wonl, dot dothem to mhethem to ler'efor toem od e s an' n' gy fo them oem\n",
      "ny onkwoom to lo  d tod ep, don'tod am ap port e th  them th lo lent wonc and ton'tod s gp them to  s aod ton , dht d them th thethem th lon' for them nd e   ant ns ty ay them osm\n",
      "nf onkwoad to lo ld a t ep, don'toa at an tert e th lyher th co lent wont and won't a s gn them to    and wonk, dot a ther th thethem to len' fon them nd e   an  ns ty oy them hsp\n",
      "tf onkwoad to cu ld aotsit, don't aout an fent eeth lyher th cu le t wont and ton't ans gn them tosss and wonk, dut o ther to th them to cong fon themwnd e s ens ns ty on themw s \n",
      "tf onkwoud to cu ld a them, don'toaoum un fontle th ether to lu le t wonc and won't a s gn them tosss and wonk, dut uother to th them to bu g tor themwnd e s ens nsity on themw sm\n",
      "tp orkwoud to bu ld a shem, dor'toaaum um portle th lther to cu le t eonm ard won't a s gn them tosss a d work, but uother to mhethem to cong tor themwnd e s ems nsit  on themwhim\n",
      "tp orkwond to bo ld d shem, dor't daum un pentle th lther to bo le t wonm ard won't wrs gn them tosss and work, but rather to mh them to lorg tor themwnd e s ems nsity oo themwhim\n",
      "tp orkwond to bo ld d shep, don't waum rp penple to lther to bo le t wonp and won't wnsign them tosss and won , but rather tonch them to lorg por themend ess emm nsity of themehim\n",
      "tp oukwond to bo ld a shep, don't aaum uf penplecto ether to bo lect wonm and don't wssign them tosks and don , but rather tonch them to bong for toemendless imm nsity of themehic\n",
      "tp ou wond to lo ld a shep, don't aaum uf penpleeto ether to lollect wonm and won't assign them tosks and won , but rather toich them to lorg for themend ess imm nsity of themehic\n",
      "tp ou wond to luild a ship, don't aaum up penplectogether to lollect woom and won't assign them tosks and wonk, but rather toich them to lorg for themend ess imm nsity of themehic\n",
      "tpyou wont to boild a ship, don't daum up penplectoglther to co lect woom and won't assign them tosks and work, but rather tosch them to bong for themend ess imm nsity of themehim\n",
      "tpyoo wond to build a ship, don't drum up penple together to collect woom and won't assign them tosks and work, but rather tosch them to long for themend ess imm nsity of themehic\n",
      "tpyookwont to luild a ship, don't arum up penple together to lollect woom and don't assign them tosks and work, but rather toach them to lorg for themend ess imm nsity of themehic\n",
      "tpyookwont to build a ship, don't arum up pentle together to collect woot and won't assign them tosks and work, but rother toach them to lorg for themendless imm nsity of themenic\n",
      "t toukwont to build a ship, don't arum up penple together to collect woot and don't assign them tosks and work, but rother toach them to cong for themendless immensity af the ehac\n",
      "t tou wont to build a ship, don't arum up penple together to collect woot and don't assign them tosks and work, but rather toach them to cong for themendless immensity af themeeac\n",
      "t too wont to build a ship, don't arum up penple together to collect woom and don't assign them tosks and work, but rather toach them to long for themendless immensity of themeeac\n",
      "t tou want to build a ship, don't arum up penple together to lollect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the seac\n",
      "t you want to build a ship, don't arum up penple together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the seac\n",
      "t you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the eeac\n",
      "t you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the seac\n",
      "t you want to build a ship, don't arum up people together to collect wood and won't assign them tosks and work, but rather toach them to long for the endless immensity of the eeac\n",
      "l you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and dork, but rather toach them to long for the endless immensity of the seac\n",
      "l you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the eeac\n",
      "l you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the seac\n",
      "l you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the sndless immensity of the sea.\n",
      "p you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather toach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather toach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach them ta long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach them ta long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather toach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather toach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather teach them to long for the sndless immensity of the seas\n",
      "l you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seac\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
     ]
    }
   ],
   "source": [
    "# 문자 단위 RNN(Char RNN) - 더 많은 데이터\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "            \"collect wood and don't assign them tasks and work, but rather \"\n",
    "            \"teach them to long for the endless immensity of the sea.\")\n",
    "\n",
    "char_set = list(set(sentence))  # 중복을 제거한 문자 집합 생성\n",
    "char_dic = {c:i for i,c in enumerate(char_set)}  # 각 문자에 정수 인코딩\n",
    "print(char_dic)  # 공백도 하나의 원소로 침\n",
    "dic_size = len(char_dic)\n",
    "\n",
    "# 하이퍼파라이터 설정\n",
    "hidden_size = dic_size\n",
    "sequence_length = 10  # 10개 문자 단위로 잘라 샘플을 만들것임\n",
    "learning_rate = 0.1\n",
    "\n",
    "# 샘플 만들기\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    x_str = sentence[i:i+sequence_length]\n",
    "    y_str = sentence[i+1:i+sequence_length+1]\n",
    "    # print(i, x_str, '->', y_str)\n",
    "    \n",
    "    x_data.append([char_dic[c] for c in x_str])\n",
    "    y_data.append([char_dic[c] for c in y_str])\n",
    "    \n",
    "print(x_data[0])  # if you wan에 해당됨.\n",
    "print(y_data[0])  # f you want에 해당됨.\n",
    "\n",
    "x_one_hot = [np.eye(dic_size)[x] for x in x_data]  # 입력 시퀀스 원-핫 인코딩\n",
    "X = torch.FloatTensor(x_one_hot)  # 텐서로 변환\n",
    "Y = torch.LongTensor(y_data)\n",
    "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
    "print('레이블의 크기 : {}'.format(Y.shape))\n",
    "print(X[0])  \n",
    "print(Y[0])  #  f you want에 해당\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layers): \n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "net = Net(dic_size, hidden_size, 2)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X) # (170, 10, 25)크기의 텐서를 매 epoch마다 모델의 입력으로 사용\n",
    "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1)) # [1700, 25]와 [1700]크기 비교\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # results의 텐서 크기는 (170, 10)\n",
    "    results = outputs.argmax(dim=2)\n",
    "    predict_str = \"\"\n",
    "    for j, result in enumerate(results):\n",
    "        if j == 0: # 처음에는 예측 결과를 전부 가져오지만\n",
    "            predict_str += ''.join([char_set[t] for t in result])\n",
    "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
    "            predict_str += char_set[result[-1]]\n",
    "\n",
    "    print(predict_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ff9edcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Repeat', 'is', 'the', 'best', 'medicine', 'for', 'memory']\n",
      "['the', 'memory', 'Repeat', 'medicine', 'is', 'best', 'for']\n",
      "{'the': 1, 'memory': 2, 'Repeat': 3, 'medicine': 4, 'is': 5, 'best': 6, 'for': 7, '<unk>': 0}\n",
      "{1: 'the', 2: 'memory', 3: 'Repeat', 4: 'medicine', 5: 'is', 6: 'best', 7: 'for', 0: '<unk>'}\n",
      "tensor([[3, 5, 1, 6, 4, 7]])\n",
      "tensor([[5, 1, 6, 4, 7, 2]])\n",
      "[01/201] 2.1572 \n",
      "Repeat <unk> <unk> memory <unk> Repeat <unk>\n",
      "\n",
      "[41/201] 1.4645 \n",
      "Repeat for the best medicine for memory\n",
      "\n",
      "[81/201] 0.8481 \n",
      "Repeat is the best medicine for memory\n",
      "\n",
      "[121/201] 0.4352 \n",
      "Repeat is the best medicine for memory\n",
      "\n",
      "[161/201] 0.2322 \n",
      "Repeat is the best medicine for memory\n",
      "\n",
      "[201/201] 0.1422 \n",
      "Repeat is the best medicine for memory\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 단어 단위 RNN - 임베딩 사용\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 임의의 문장\n",
    "# 'Repeat is the best medicine for'을 입력받으면 'is the best medicine for memory'를 출력하는 RNN 만들기\n",
    "sentence = \"Repeat is the best medicine for memory\".split()\n",
    "print(sentence)\n",
    "\n",
    "# 단어장 만들기\n",
    "vocab = list(set(sentence))\n",
    "print(vocab)\n",
    "word2index = {tnk:i for i,tnk in enumerate(vocab, 1)}  # 단어에 고유한 정수 부여\n",
    "word2index['<unk>'] = 0  # 모르는 단어를 의미하는 UNK 토큰 추가\n",
    "print(word2index)\n",
    "\n",
    "index2word = {v:k for k,v in word2index.items()}\n",
    "print(index2word)\n",
    "\n",
    "# 데이터의 각 단어를 정수로 인코딩\n",
    "def build_data(sentence, word2index):\n",
    "    encoded = [word2index[token] for token in sentence]  # 각 문자를 정수로 변환\n",
    "    input_seq, label_seq = encoded[:-1], encoded[1:]  # 입력 시퀀스, 레이블 시퀀스\n",
    "    \n",
    "    input_seq = torch.LongTensor(input_seq).unsqueeze(0)  # 배치 차원 추가\n",
    "    label_seq = torch.LongTensor(label_seq).unsqueeze(0)  # 배치 차원 추가\n",
    "    \n",
    "    return input_seq, label_seq\n",
    "\n",
    "X, Y = build_data(sentence, word2index)\n",
    "print(X)  # Repeat is the best medicine for을 의미\n",
    "print(Y)  # is the best medicine for memory을 의미\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size, input_size, hidden_size, batch_first=True):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=input_size)\n",
    "        self.rnn_layer = nn.RNN(input_size, hidden_size, batch_first=batch_first)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size) # 출력은 원-핫 벡터의 크기를 가져야함. 또는 단어 집합의 크기만큼 가져야함.\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 1. 임베딩 층\n",
    "        # 크기변화: (배치 크기, 시퀀스 길이) => (배치 크기, 시퀀스 길이, 임베딩 차원)\n",
    "        output = self.embedding_layer(x)\n",
    "        \n",
    "        # 2. RNN 층\n",
    "        # 크기변화: (배치 크기, 시퀀스 길이, 임베딩 차원)\n",
    "        # => output (배치 크기, 시퀀스 길이, 은닉층 크기), hidden (1, 배치 크기, 은닉층 크기)\n",
    "        output, hidden = self.rnn_layer(output)\n",
    "        \n",
    "        # 3. 최종 출력층\n",
    "        # 크기변화: (배치 크기, 시퀀스 길이, 은닉층 크기) => (배치 크기, 시퀀스 길이, 단어장 크기)\n",
    "        output = self.linear(output)\n",
    "        \n",
    "        # 4. view를 통해서 배치 차원 제거\n",
    "        # 크기변화: (배치 크기, 시퀀스 길이, 단어장 크기) => (배치 크기*시퀀스 길이, 단어장 크기)\n",
    "        \n",
    "        return output.view(-1, output.size(2))\n",
    "\n",
    "    \n",
    "# 하이퍼 파라미터\n",
    "vocab_size = len(word2index)  # 단어장의 크기는 임베딩 층, 최종 출력층에 사용\n",
    "input_size = 5  # 임베딩 된 차원의 크기 및 RNN 층 입력 차원의 크기\n",
    "hidden_size = 20  # RNN의 은닉층 크기\n",
    "\n",
    "model = Net(vocab_size, input_size, hidden_size, batch_first=True)\n",
    "loss_function = nn.CrossEntropyLoss() # 소프트맥스 함수 포함\n",
    "optimizer = optim.Adam(params=model.parameters())\n",
    "\n",
    "# 수치화된 데이터를 단어로 전환하는 함수\n",
    "decode = lambda y: [index2word.get(x) for x in y]\n",
    "\n",
    "for step in range(201):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X)\n",
    "\n",
    "    loss = loss_function(output, Y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  \n",
    "    if step % 40 == 0:\n",
    "        print(\"[{:02d}/201] {:.4f} \".format(step+1, loss))\n",
    "        pred = output.softmax(-1).argmax(-1).tolist()\n",
    "        print(\" \".join([\"Repeat\"] + decode(pred)))\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
