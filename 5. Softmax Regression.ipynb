{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e7280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소프트맥스 회귀\n",
    "\n",
    "'''\n",
    "다중 클래스 분류: 3개 이상의 선택지로부터 1개를 선택\n",
    "소프트맥스 회귀: 다중 클래스 분류를 풀기위한 알고리즘\n",
    "\n",
    "소프트맥스 함수\n",
    "- 각 선택지가 정답일 확률로 표현 \n",
    "- 확률의 총 합이 1\n",
    "- 선택지의 개수만큼의 차원을 가지는 벡터를 만들고, 해당 벡터가 벡터의 모든 원소의 합이 1이 되도록 원소들의 값을 변환\n",
    "\n",
    "즉, \n",
    "선형회귀의 가설은 H(x) = Wx+b, \n",
    "로지스틱 회귀의 가설은 H(x) = sigmoid(Wx+b) 0~1사이의 하나의 값으로 표현,\n",
    "소프트맥스 회귀의 가설은 H(x) = softmax(Wx+b)\n",
    "\n",
    "소프트맥스 회귀에서는 실제값을 원-핫 벡터로 표현\n",
    "원-핫 인코딩은 선택해야 하는 선택지의 개수만큼의 차원을 가지면서, \n",
    "각 선택지의 인덱스에 해당하는 원소에는 1, \n",
    "나머지 원소는 0의 값을 가지도록 하는 표현 방법\n",
    "\n",
    "비용함수 - cross entropy 함수\n",
    "cross entropy = -(y1log(p1)+y2log(p2)+y3log(y3)+ ...)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14138c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# cross entropy 함수 구현\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 3개의 원소를 가진 벡터 텐서를 정의\n",
    "z = torch.FloatTensor([1, 2, 3])\n",
    "\n",
    "# 소프트맥스 함수\n",
    "hypothesis = F.softmax(z, dim=0)\n",
    "print(hypothesis)\n",
    "# 원소의 값이 0과 1사이의 값을 가지는 벡터로 변환됨\n",
    "\n",
    "# 원소들의 값의 합이 1인지 확인\n",
    "print(hypothesis.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "621ac6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2974, 0.2244, 0.1595, 0.1420, 0.1767],\n",
      "        [0.1509, 0.1595, 0.1917, 0.2522, 0.2457],\n",
      "        [0.1002, 0.2226, 0.1878, 0.2624, 0.2271]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([3, 2, 3])\n",
      "tensor([[0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.]])\n",
      "tensor(1.6471, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 소프트맥스 함수 - 직접 구현\n",
    "# torch.log(F.softmax(z, dim=1))\n",
    "\n",
    "# 각 샘플에 대해서 소프트맥스 함수를 적용\n",
    "z = torch.rand(3, 5, requires_grad=True)\n",
    "hypothesis = F.softmax(z, dim=1)\n",
    "print(hypothesis)\n",
    "\n",
    "# 각 샘플에 대해서 임의의 레이블을 만들기\n",
    "y = torch.randint(5, (3,)).long()\n",
    "print(y)\n",
    "\n",
    "# 각 레이블에 대해서 원-핫 인코딩을 수행\n",
    "y_one_hot = torch.zeros_like(hypothesis) # 모든 원소가 0의 값을 가진 3 × 5 텐서 생성\n",
    "y_one_hot.scatter_(1, y.unsqueeze(1), 1)\n",
    "print(y_one_hot)\n",
    "\n",
    "# 소프트맥스 회귀의 비용 함수 계산\n",
    "cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc9824a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4])\n",
      "torch.Size([8])\n",
      "tensor([[0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.]])\n",
      "torch.Size([8, 3])\n",
      "Epoch    0/1000 Cost: 1.098612\n",
      "Epoch  100/1000 Cost: 0.761050\n",
      "Epoch  200/1000 Cost: 0.689991\n",
      "Epoch  300/1000 Cost: 0.643229\n",
      "Epoch  400/1000 Cost: 0.604117\n",
      "Epoch  500/1000 Cost: 0.568255\n",
      "Epoch  600/1000 Cost: 0.533922\n",
      "Epoch  700/1000 Cost: 0.500291\n",
      "Epoch  800/1000 Cost: 0.466908\n",
      "Epoch  900/1000 Cost: 0.433507\n",
      "Epoch 1000/1000 Cost: 0.399962\n"
     ]
    }
   ],
   "source": [
    "# 소프트맥스 회귀\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 데이터\n",
    "x_train = [[1, 2, 1, 1],\n",
    "           [2, 1, 3, 2],\n",
    "           [3, 1, 3, 4],\n",
    "           [4, 1, 5, 5],\n",
    "           [1, 7, 5, 5],\n",
    "           [1, 2, 5, 6],\n",
    "           [1, 6, 6, 6],\n",
    "           [1, 7, 7, 7]]\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "y_one_hot = torch.zeros(8,3)\n",
    "y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\n",
    "print(y_one_hot)\n",
    "print(y_one_hot.shape)\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros((4,3), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W,b], lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    y_pred = F.softmax(x_train.matmul(W) + b, dim=1)\n",
    "    \n",
    "    cost = (y_one_hot * -torch.log(y_pred)).sum(dim=1).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4be75565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4])\n",
      "torch.Size([8])\n",
      "Epoch    0/1000 Cost: 1.098612\n",
      "Epoch  100/1000 Cost: 0.761050\n",
      "Epoch  200/1000 Cost: 0.689991\n",
      "Epoch  300/1000 Cost: 0.643229\n",
      "Epoch  400/1000 Cost: 0.604117\n",
      "Epoch  500/1000 Cost: 0.568255\n",
      "Epoch  600/1000 Cost: 0.533922\n",
      "Epoch  700/1000 Cost: 0.500291\n",
      "Epoch  800/1000 Cost: 0.466908\n",
      "Epoch  900/1000 Cost: 0.433507\n",
      "Epoch 1000/1000 Cost: 0.399962\n"
     ]
    }
   ],
   "source": [
    "# 소프트맥스 회귀 - 함수 사용\n",
    "\n",
    "'''\n",
    "위에서는 소프트맥스 함수의 출력값을 로그 함수의 입력으로 사용\n",
    "torch.log(F.softmax(z, dim=1))\n",
    "cost = (y_one_hot * -torch.log(F.softmax(z, dim=1))).sum(dim=1).mean()\n",
    "\n",
    "파이토치에서는 두 개의 함수를 결합한 F.log_softmax()라는 도구를 제공\n",
    "F.log_softmax(z, dim=1)\n",
    "cost = (y_one_hot * - F.log_softmax(z, dim=1)).sum(dim=1).mean()\n",
    "\n",
    "F.nll_loss()를 사용할 때는 원-핫 벡터를 넣을 필요없이 바로 실제값을 인자로 사용\n",
    "cost = F.nll_loss(F.log_softmax(z, dim=1), y)\n",
    "\n",
    "F.cross_entropy()는 F.log_softmax()와 F.nll_loss()를 포함\n",
    "cost = F.cross_entropy(z, y)\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 데이터\n",
    "x_train = [[1, 2, 1, 1],\n",
    "           [2, 1, 3, 2],\n",
    "           [3, 1, 3, 4],\n",
    "           [4, 1, 5, 5],\n",
    "           [1, 7, 5, 5],\n",
    "           [1, 2, 5, 6],\n",
    "           [1, 6, 6, 6],\n",
    "           [1, 7, 7, 7]]\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros((4,3), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W,b], lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    y_pred = x_train.matmul(W) + b\n",
    "    \n",
    "    # F.cross_entropy()는 그 자체로 소프트맥스 함수를 포함\n",
    "    # 가설에서 소프트맥스 함수를 사용할 필요가 없음\n",
    "    cost = F.cross_entropy(y_pred, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aba7e59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.616785\n",
      "Epoch  100/1000 Cost: 0.658891\n",
      "Epoch  200/1000 Cost: 0.573444\n",
      "Epoch  300/1000 Cost: 0.518151\n",
      "Epoch  400/1000 Cost: 0.473265\n",
      "Epoch  500/1000 Cost: 0.433516\n",
      "Epoch  600/1000 Cost: 0.396563\n",
      "Epoch  700/1000 Cost: 0.360914\n",
      "Epoch  800/1000 Cost: 0.325392\n",
      "Epoch  900/1000 Cost: 0.289178\n",
      "Epoch 1000/1000 Cost: 0.254148\n"
     ]
    }
   ],
   "source": [
    "# 소프트맥스 회귀 - nn.Module로 구현하기\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 데이터\n",
    "x_train = [[1, 2, 1, 1],\n",
    "           [2, 1, 3, 2],\n",
    "           [3, 1, 3, 4],\n",
    "           [4, 1, 5, 5],\n",
    "           [1, 7, 5, 5],\n",
    "           [1, 2, 5, 6],\n",
    "           [1, 6, 6, 6],\n",
    "           [1, 7, 7, 7]]\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "\n",
    "# 선형 회귀에서 구현에 사용했던 nn.Linear()를 사용\n",
    "model = nn.Linear(4,3)  # 4개의 특성을 가지고 3개의 클래스로 분류. input_dim=4, output_dim=3\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1154828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 2.637636\n",
      "Epoch  100/1000 Cost: 0.647903\n",
      "Epoch  200/1000 Cost: 0.564643\n",
      "Epoch  300/1000 Cost: 0.511043\n",
      "Epoch  400/1000 Cost: 0.467249\n",
      "Epoch  500/1000 Cost: 0.428281\n",
      "Epoch  600/1000 Cost: 0.391924\n",
      "Epoch  700/1000 Cost: 0.356742\n",
      "Epoch  800/1000 Cost: 0.321577\n",
      "Epoch  900/1000 Cost: 0.285617\n",
      "Epoch 1000/1000 Cost: 0.250818\n"
     ]
    }
   ],
   "source": [
    "# 소프트맥스 회귀 - 클래스로 구현하기\n",
    "\n",
    "class SoftmaxRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4,3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "model = SoftmaxRegression()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32a60313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음 기기로 학습합니다: cpu\n",
      "Using downloaded and verified file: MNIST_data/MNIST\\raw\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/MNIST\\raw\\train-images-idx3-ubyte.gz to MNIST_data/MNIST\\raw\n",
      "Using downloaded and verified file: MNIST_data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/MNIST\\raw\\train-labels-idx1-ubyte.gz to MNIST_data/MNIST\\raw\n",
      "Using downloaded and verified file: MNIST_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to MNIST_data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26e5c12aa0f40c6b216a0920b918db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST\\raw\n",
      "Processing...\n",
      "Done!\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001BF3B85A1C8>\n",
      "Epoch: 0001 cost = 0.535468519\n",
      "Epoch: 0002 cost = 0.359274209\n",
      "Epoch: 0003 cost = 0.331187546\n",
      "Epoch: 0004 cost = 0.316578090\n",
      "Epoch: 0005 cost = 0.307158172\n",
      "Epoch: 0006 cost = 0.300180763\n",
      "Epoch: 0007 cost = 0.295130253\n",
      "Epoch: 0008 cost = 0.290851533\n",
      "Epoch: 0009 cost = 0.287417084\n",
      "Epoch: 0010 cost = 0.284379542\n",
      "Epoch: 0011 cost = 0.281825215\n",
      "Epoch: 0012 cost = 0.279800713\n",
      "Epoch: 0013 cost = 0.277808994\n",
      "Epoch: 0014 cost = 0.276154310\n",
      "Epoch: 0015 cost = 0.274440855\n",
      "Learning finished\n",
      "Accuracy: 0.8863000273704529\n",
      "Label:  8\n",
      "Prediction:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Networking_Lab_009\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:60: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "C:\\Users\\Networking_Lab_009\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:50: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 소프트맥스 회귀로 MNIST 데이터 분류하기\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available() # GPU를 사용가능하면 True, 아니라면 False를 리턴\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\") # GPU 사용 가능하면 사용하고 아니면 CPU 사용\n",
    "print(\"다음 기기로 학습합니다:\", device)\n",
    "\n",
    "# for reproducibility\n",
    "random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "    \n",
    "# hyperparameters\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# MNIST 데이터셋을 불러오기\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',  # 저장할 경로\n",
    "                          train=True,  # train data 불러오기\n",
    "                          transform=transforms.ToTensor(),  # 현재 데이터를 파이토치 텐서로 변환\n",
    "                          download=True)  # 해당 root 경로에 MNIST 데이터가 없다면 다운로드\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)\n",
    "\n",
    "# 데이터 미니배치\n",
    "data_loader = DataLoader(dataset=mnist_train,  # 로드할 대상\n",
    "                        batch_size=batch_size,   # 배치 크기는 100\n",
    "                        shuffle = True,  # 매 epoch마다 셔플\n",
    "                        drop_last = True  # 마지막 배치 버리기 (상대적으로 과대 평가되는 것을 막아줌)\n",
    "                        )\n",
    "print(data_loader)\n",
    "\n",
    "# 모델 설계\n",
    "# MNIST data image of shape 28 * 28 = 784\n",
    "linear = nn.Linear(784, 10, bias=True).to(device)  # to()는 연산을 어디서 할지 정함\n",
    "# bias는 편향 term을 사용할 것인지 정함\n",
    "\n",
    "# 비용 함수와 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss().to(device)  # # 내부적으로 소프트맥스 함수를 포함하고 있음\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader)\n",
    "    \n",
    "    for X, Y in data_loader:\n",
    "        # 배치 크기가 100이므로 아래의 연산에서 X는 (100, 784)의 텐서\n",
    "        X = X.view(-1, 28*28).to(device)\n",
    "        # 레이블은 원-핫 인코딩이 된 상태가 아니라 0 ~ 9의 정수\n",
    "        Y = Y.to(device)\n",
    "        \n",
    "        y_pred = linear(X)\n",
    "        cost = criterion(y_pred, Y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost += cost / total_batch\n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')\n",
    "\n",
    "# 테스트 데이터를 사용하여 모델을 테스트\n",
    "with torch.no_grad(): # torch.no_grad()를 하면 gradient 계산을 수행하지 않음\n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "\n",
    "    prediction = linear(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())\n",
    "\n",
    "    # MNIST 테스트 데이터에서 무작위로 하나를 뽑아서 예측\n",
    "    r = random.randint(0, len(mnist_test) - 1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
    "\n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = linear(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())\n",
    "\n",
    "    plt.imshow(mnist_test.test_data[r:r + 1].view(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
